{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob #filenames and pathnames utility\n",
    "import os   #operating sytem utility\n",
    "import warnings\n",
    "\n",
    "import flowgatenist as flow\n",
    "#from flowgatenist import gaussian_mixture as nist_gmm\n",
    "import flowgatenist.batch_process as batch_p\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib.dates\n",
    "#from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "#from scipy import special\n",
    "#from scipy import misc\n",
    "\n",
    "import cmdstanpy\n",
    "import gsf_ims_fitness.stan_utility as stan_utility\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set global default style:\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\", {'xtick.direction':'in', 'xtick.top':True, 'ytick.direction':'in', 'ytick.right':True, })\n",
    "#sns.set_style({\"axes.labelsize\": 20, \"xtick.labelsize\" : 16, \"ytick.labelsize\" : 16})\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "plt.rcParams['legend.edgecolor'] = 'k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicate the directory where the notebook is saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "notebook_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = notebook_dir[:notebook_dir.rfind(\"\\\\\")]\n",
    "main_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(main_directory)\n",
    "data_directories = glob.glob('*_Cytom*')\n",
    "data_directories.sort()\n",
    "data_directories = np.array(data_directories)\n",
    "data_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "summaries = []\n",
    "mean_summaries = []\n",
    "for direct in data_directories:\n",
    "    for plate_str in ['plate_1', 'plate_2']:\n",
    "        os.chdir(main_directory)\n",
    "        os.chdir(direct)\n",
    "        try:\n",
    "            os.chdir(plate_str)\n",
    "            plate_pickle_file = direct + f'_{plate_str}_summary.frame_pkl'\n",
    "            \n",
    "            try:\n",
    "                plate_layout = pickle.load(open(plate_pickle_file, 'rb'))\n",
    "                summaries.append(plate_layout)\n",
    "            except:\n",
    "                print(f'file not found: {plate_pickle_file}')\n",
    "        except:\n",
    "            print(f'plate not found: {direct}, {plate_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_summary = pd.concat(summaries, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(full_summary.plasmid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clone_des_list = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "parent_list = []\n",
    "variant_list = []\n",
    "clone_list = []\n",
    "for x in full_summary['plasmid']:\n",
    "    clone_str = ''\n",
    "    for c in clone_des_list:\n",
    "        if x[-len(c):] == c:\n",
    "            clone_str = c\n",
    "            break\n",
    "    clone_list.append(clone_str)\n",
    "    \n",
    "    if len(clone_str) > 0:\n",
    "        x = x[:-len(clone_str)]\n",
    "        if x[-1] == '-': x = x[:-1]\n",
    "    variant_list.append(x)\n",
    "    \n",
    "    par_p = x[:x.find('(')] if '(' in x else x\n",
    "    parent_list.append(par_p)\n",
    "    \n",
    "parent_list = [x.rstrip('-') for x in  parent_list]\n",
    "variant_list = [x.rstrip('-') for x in  variant_list]\n",
    "full_summary['parent_plasmid'] = parent_list\n",
    "full_summary['clone'] = clone_list\n",
    "full_summary['variant'] = variant_list\n",
    "\n",
    "parent_plasmids = np.unique(full_summary['parent_plasmid'])\n",
    "parent_plasmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(full_summary.variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_plate = []\n",
    "for file in full_summary['coli_file']:\n",
    "    if file == '':\n",
    "        date_plate.append('')\n",
    "    else:\n",
    "        date = file[:file.find('_')]\n",
    "        plate = file[file.find('plate'):]\n",
    "        plate = plate[:plate.find('_')]\n",
    "        date_plate.append(f'{date}_{plate}')\n",
    "full_summary['date_plate'] = date_plate\n",
    "np.unique(date_plate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of bad replicates (e.g., outliers) to ignore in quantitative analysis\n",
    "bad_reps_list = [] # [['2021-01-20', 'plate-2', 'pVER-IPTG-002(F161V)B'],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bad_rep = []\n",
    "for ind, row in full_summary.iterrows():\n",
    "    is_bad = False\n",
    "    for bc in bad_reps_list:\n",
    "        if (row.coli_file.startswith(bc[0]))&(bc[1] in row.coli_file)&(row.plasmid==bc[2]):\n",
    "            is_bad = is_bad | True\n",
    "    bad_rep.append(is_bad)\n",
    "full_summary['is_bad_rep'] = bad_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_summary\n",
    "df = df[~df.is_bad_rep]\n",
    "np.unique(df.inducerConcentration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligand_list = np.unique(full_summary.inducerId)\n",
    "ligand_list = ligand_list[ligand_list!='none']\n",
    "ligand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix non-uniformity in ligand naming:\n",
    "new_ligand = []\n",
    "for ligand in full_summary.inducerId:\n",
    "    if ligand == '4-Et-Guai':\n",
    "        ligand = '4-ethylguaiacol'\n",
    "    elif ligand == '4-Eth-Phen':\n",
    "        ligand = '4-ethylphenol'\n",
    "    elif (ligand == '4-V-Phen') or (ligand == '4vinylPhenol'):\n",
    "        ligand = '4-vinylphenol'\n",
    "    elif (ligand == 'Sorbose'):\n",
    "        ligand = 'L-sorbose'\n",
    "    ligand = ligand.replace('Arabitol', 'arabitol')\n",
    "    ligand = ligand.replace('Ribose', 'ribose')\n",
    "    ligand = ligand.replace('Eugenol', 'eugenol')\n",
    "    ligand = ligand.replace('Isoeugenol', 'isoeugenol')\n",
    "    new_ligand.append(ligand)\n",
    "full_summary['inducerId'] = new_ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligand_list = np.unique(full_summary.inducerId)\n",
    "ligand_list = ligand_list[ligand_list!='none']\n",
    "ligand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [6, 4]\n",
    "fig, axs = plt.subplots()\n",
    "\n",
    "axs.hist(np.log10(full_summary.singlet_count), bins=50);\n",
    "axs.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark any additional outlier points, either manually (by date_plate and well) or by singlet_count cutoff\n",
    "outlier_points = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#full_summary['is_outlier'] = False\n",
    "\n",
    "count_cutoff = 10**4\n",
    "\n",
    "outlier_list = []\n",
    "for ind, row in full_summary.iterrows():\n",
    "    is_outlier = False #row.is_outlier\n",
    "    if row.singlet_count<count_cutoff:\n",
    "        is_outlier = True\n",
    "    else:\n",
    "        for point in outlier_points:\n",
    "            if (row.date_plate==point[0]) and (row.well==point[1]):\n",
    "                is_outlier = True\n",
    "    outlier_list.append(is_outlier)\n",
    "full_summary['is_outlier'] = outlier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_summary\n",
    "df = df[df.is_outlier]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot cytometry data for every variant to see if there are any remaining bad clones\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 4]\n",
    "for var in np.unique(full_summary['variant']):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    fig.suptitle(var, size=20)\n",
    "    fig.suptitle(var, size=20)\n",
    "    df_0 = full_summary\n",
    "    df_0 = df_0[df_0.variant==var]\n",
    "    plate_list = np.unique(df_0.date_plate)\n",
    "    \n",
    "    lig_list = np.unique(df_0.inducerId)\n",
    "    if var == 'pAN-1201':\n",
    "        lig_list = ['none']\n",
    "        min_x = 1\n",
    "    else:\n",
    "        lig_list = lig_list[lig_list!='none']\n",
    "\n",
    "        min_x = df_0.inducerConcentration\n",
    "        min_x = min_x[min_x>0]\n",
    "        min_x = min(min_x)\n",
    "    \n",
    "    for p in plate_list:\n",
    "        for clone in np.unique(df_0['clone']):\n",
    "            df_1 = df_0[df_0.date_plate==p]\n",
    "            df_1 = df_1[df_1['clone']==clone]\n",
    "            \n",
    "            for lig, fmt in zip(lig_list, ['-o', '-^', '-v', '->']):\n",
    "                for ax in axs:\n",
    "                    df = df_1[(df_1.inducerId==lig)|(df_1.inducerId=='none')]\n",
    "                    df_non_zero = df_1[(df_1.inducerId==lig)]\n",
    "                    if len(df_non_zero)>0:\n",
    "                        label = f'{p}'\n",
    "                        label = f'{label}.{clone}' if (clone !='') else label\n",
    "                        label = f'{label}, {lig}'\n",
    "\n",
    "                        if df.is_bad_rep.iloc[0]:\n",
    "                            label = f'{label} - bad rep'\n",
    "                            fillstyle = 'none'\n",
    "                        else:\n",
    "                            fillstyle = None\n",
    "                        x = df.inducerConcentration\n",
    "                        y = df['mean']\n",
    "                        ax.plot(x, y, fmt, label=label, fillstyle=fillstyle, ms=12)\n",
    "                        \n",
    "                        df = df[df.is_outlier]\n",
    "                        x = df.inducerConcentration\n",
    "                        y = df['mean']\n",
    "                        ax.plot(x, y, fmt[-1], color='k', fillstyle='none', ms=20)\n",
    "                    \n",
    "                \n",
    "    x_label = '[ligand]'\n",
    "    for ax in axs:\n",
    "        ax.set_xscale('symlog', linthresh=min_x/4);\n",
    "        ax.set_xlabel(f'{x_label} (umol/L)', size=14)\n",
    "    ax.set_yscale('log');\n",
    "    ax.legend(loc='upper left', bbox_to_anchor= (1.1, 1.1), ncol=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(notebook_dir)\n",
    "file_name = 'Ligafy_cytom_data_all'\n",
    "with open(f'{file_name}.pkl', 'wb') as file:\n",
    "    pickle.dump(full_summary, file)\n",
    "    \n",
    "full_summary.to_csv(f'{file_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(full_summary.variant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(full_summary.variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_stan_fit(x_data, y_data, mid_start=None):\n",
    "    x_max = max(x_data)\n",
    "    x_min = min(x_data)\n",
    "    \n",
    "    if mid_start is None:\n",
    "        x_non_zero = x[x>0]\n",
    "        mid_start = np.exp(np.mean(np.log(x_non_zero)))\n",
    "    \n",
    "    low = np.mean(y_data[x_data<=x_min])\n",
    "    high = np.mean(y_data[x_data>=x_max])\n",
    "    mid = np.random.normal(1, 0.2) * mid_start\n",
    "    n = np.random.normal(1, 0.2) * 1.1\n",
    "    sig = np.random.normal(1, 0.2) * 100\n",
    "    \n",
    "    return dict(log_low_level=np.log10(low), log_high_level=np.log10(high), \n",
    "                log_IC_50=np.log10(mid), sensor_n=n,\n",
    "                sigma=sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stan_model_file = \"C:\\\\Users\\\\djross\\\\Documents\\\\Python Scripts\\\\gsf_ims_fitness\\\\gsf_ims_fitness\\\\Stan models\\\\\"\n",
    "stan_model_file = \"Hill equation fit.stan\"\n",
    "# Load Stan model\n",
    "stan_model = stan_utility.compile_model(stan_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(full_summary['mean'].min(), full_summary['mean'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_summary\n",
    "df = df[df.variant=='pAN-1201']\n",
    "\n",
    "non_fluorescent_mean = df['mean'].mean()\n",
    "non_fluorescent_mean_err = df['mean'].std()\n",
    "\n",
    "ok_to_fail_fit = []\n",
    "non_fluorescent_mean, non_fluorescent_mean_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_summary\n",
    "df = df[df['mean']<non_fluorescent_mean]\n",
    "np.unique(df.variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_summary\n",
    "df = df[df.variant!='pAN-1201']\n",
    "df['mean'].min(), df['mean'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in np.unique(full_summary.variant):\n",
    "    f = full_summary\n",
    "    f = f[f['variant']==var]\n",
    "    f = f[~f.is_bad_rep]\n",
    "    f = f[~f.is_outlier]\n",
    "    \n",
    "    y = f['mean'] - non_fluorescent_mean\n",
    "    print(var, min(y), max(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmin = 10\n",
    "gmax = {}\n",
    "gmax['pLigify-IemR'] = 30000\n",
    "gmax['pLigify-LcLacI'] = 150000\n",
    "gmax['pLigify-SorR'] = 3000\n",
    "gmax['pLigify-VprR'] = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_summary\n",
    "df = df[df.variant!='pAN-1201']\n",
    "np.unique(df[df['mean']<300].variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fits = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior on sensor_n; <gamma> = alpha/beta = 2; std = sqrt(alpha)/beta = 1.2\n",
    "sensor_n_alpha = 3\n",
    "sensor_n_beta = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_n_alpha/sensor_n_beta, np.sqrt(sensor_n_alpha)/sensor_n_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "cmdstanpy_logger = logging.getLogger(\"cmdstanpy\")\n",
    "cmdstanpy_logger.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(full_summary.inducerConcentration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#max_inducer = 10**20\n",
    "\n",
    "os.chdir(notebook_dir)\n",
    "pickle_file = 'Ligafy_cytom_Hill_fits_mean.pkl'\n",
    "model_pickle_file = 'Ligafy_cytom_Hill_fits_stan_model.pkl'\n",
    "if new_fits:\n",
    "    cytom_Hill_fits = {}\n",
    "else:\n",
    "    pickled_model = pickle.load(open(model_pickle_file, 'rb'))\n",
    "    cytom_Hill_fits = pickle.load(open(pickle_file, 'rb'))\n",
    "\n",
    "for i, r in enumerate(np.unique(full_summary.variant)):\n",
    "#for i, r in enumerate(['pLigafy-D2']):\n",
    "    stan_iter = 2000\n",
    "    if r =='pAN-1201': # Edit this line when we measure the zero-fluorescence control\n",
    "        run_fit = False\n",
    "    else:\n",
    "        f = full_summary\n",
    "        f = f[f['variant']==r]\n",
    "        f = f[~f.is_bad_rep]\n",
    "        #f = f[f['inducerConcentration']<max_inducer]\n",
    "        f = f[~f.is_outlier]\n",
    "        \n",
    "        lig_list = np.unique(f.inducerId)\n",
    "        lig_list = lig_list[lig_list!='none']\n",
    "        \n",
    "        for lig in lig_list:\n",
    "            df = f[(f.inducerId==lig)|(f.inducerId=='none')]\n",
    "            df_non_zero = f[(f.inducerId==lig)]\n",
    "            include_fit = (len(df_non_zero)>4)&(~np.all(df['mean'].isnull()))\n",
    "            if include_fit:\n",
    "                x = df['inducerConcentration']\n",
    "                y = df['mean'] - non_fluorescent_mean\n",
    "                yerr = df['mean_err']\n",
    "                x = x[~y.isnull()]\n",
    "                yerr = yerr[~y.isnull()]\n",
    "                y = y[~y.isnull()]\n",
    "                \n",
    "                yerr[x>=500] *= 2\n",
    "\n",
    "            if (r, lig) in cytom_Hill_fits.keys():\n",
    "                #print(f\"{i},    existing fit: {r}\")\n",
    "                run_fit = False\n",
    "                stored_fit = cytom_Hill_fits[(r, lig)]\n",
    "                # Check to see if there is new data\n",
    "                new_data = False\n",
    "                len_mismatch = len(x)!=len(stored_fit[0])\n",
    "                if len_mismatch:\n",
    "                    new_data = True\n",
    "                else:\n",
    "                    x_mismatch = (np.array(x)!=np.array(stored_fit[0])).any()\n",
    "                    y_mismatch = (np.array(y)!=np.array(stored_fit[1])).any()\n",
    "                    yerr_mismatch = (np.array(yerr)!=np.array(stored_fit[2])).any()\n",
    "                    if (x_mismatch or y_mismatch or yerr_mismatch):\n",
    "                        new_data = True\n",
    "                if new_data:\n",
    "                    run_fit = True\n",
    "                    print(f\"         non-matching data, running new fit: {r}\")\n",
    "                '''\n",
    "                # Or if diagnostics fail\n",
    "                else:\n",
    "                    if r not in ok_to_fail_fit:\n",
    "                        test1 = stan_utility.check_rhat(stored_fit[-1], rhat_threshold=1.1, verbose=False)\n",
    "                        test2 = stan_utility.check_n_eff(stored_fit[-1], ratio_threshold=0.001, verbose=False)\n",
    "                        if not (test1 and test2):\n",
    "                            run_fit = True\n",
    "                            print(f\"         old fit failed diagnostics: rhat test {test1}, n_eff test {test2}, running new fit: {r}\")\n",
    "                            stan_iter = 10000\n",
    "                '''\n",
    "                \n",
    "            else: # if there isn't a fit in the dictionary, run one\n",
    "                run_fit = True\n",
    "                \n",
    "            if run_fit:\n",
    "                if include_fit:\n",
    "                    print(f\"{i},    running fit for: {r}, {lig}\")\n",
    "                    log_x_min = min(np.log10(x[x>0])) - 0.5\n",
    "                    log_x_max = np.log10(max(x)) + 1\n",
    "                    stan_data = dict(x=x, y=y, y_err=yerr, N=len(x),\n",
    "                                     log_g_min=np.log10(gmin), log_g_max=np.log10(gmax[r]),\n",
    "                                     log_x_min=log_x_min, log_x_max=log_x_max,\n",
    "                                     sensor_n_alpha=sensor_n_alpha,\n",
    "                                     sensor_n_beta=sensor_n_beta)\n",
    "                    stan_init = init_stan_fit(x, y)\n",
    "                    stan_fit = stan_model.sample(data=stan_data, \n",
    "                                                 iter_warmup=int(stan_iter/2),\n",
    "                                                 iter_sampling=int(stan_iter/2), \n",
    "                                                 inits=stan_init, \n",
    "                                                 chains=4,\n",
    "                                                 adapt_delta=0.99, \n",
    "                                                 max_treedepth=15)\n",
    "                    cytom_Hill_fits[(r, lig)] = (x, y, yerr, stan_fit)\n",
    "                    \n",
    "                else:\n",
    "                    print(f'{i},    no data for {r}')\n",
    "                \n",
    "with open(pickle_file, 'wb') as file:\n",
    "    pickle.dump(cytom_Hill_fits, file)\n",
    "                \n",
    "with open(model_pickle_file, 'wb') as file:\n",
    "    pickle.dump(stan_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Re-run some variants\n",
    "'''\n",
    "#for i, r in enumerate(np.unique(full_summary.variant)):\n",
    "for i, r in enumerate(['pLigafy-D2-6', 'pLigafy-R2']):\n",
    "    stan_iter = 10000\n",
    "    run_fit = True\n",
    "    if r =='pAN-1201': # Edit this line when we measure the zero-fluorescence control\n",
    "        run_fit = False\n",
    "    else:\n",
    "        f = full_summary\n",
    "        f = f[f['variant']==r]\n",
    "        f = f[~f.is_bad_rep]\n",
    "        f = f[f['inducerConcentration']<max_inducer]\n",
    "        f = f[~f.is_outlier]\n",
    "        \n",
    "        if r=='pLigafy-D2-6':\n",
    "            lig = '1S-TIQ'\n",
    "        elif r=='pLigafy-R2':\n",
    "            lig = '1R-TIQ'\n",
    "            \n",
    "        df = f[(f.inducerId==lig)|(f.inducerId=='none')]\n",
    "        include_fit = (len(df)>4)&(~np.all(df['mean'].isnull()))\n",
    "        if include_fit:\n",
    "            x = df['inducerConcentration']\n",
    "            y = df['mean'] - non_fluorescent_mean\n",
    "            yerr = df['mean_err']\n",
    "            x = x[~y.isnull()]\n",
    "            yerr = yerr[~y.isnull()]\n",
    "            y = y[~y.isnull()]\n",
    "\n",
    "        if (r, lig) in cytom_Hill_fits.keys():\n",
    "            #print(f\"{i},    existing fit: {r}\")\n",
    "            #run_fit = False\n",
    "            stored_fit = cytom_Hill_fits[(r, lig)]\n",
    "            # Check to see if there is new data\n",
    "            new_data = False\n",
    "            len_mismatch = len(x)!=len(stored_fit[0])\n",
    "            if len_mismatch:\n",
    "                new_data = True\n",
    "            else:\n",
    "                x_mismatch = (np.array(x)!=np.array(stored_fit[0])).any()\n",
    "                y_mismatch = (np.array(y)!=np.array(stored_fit[1])).any()\n",
    "                yerr_mismatch = (np.array(yerr)!=np.array(stored_fit[2])).any()\n",
    "                if (x_mismatch or y_mismatch or yerr_mismatch):\n",
    "                    new_data = True\n",
    "            if new_data:\n",
    "                run_fit = True\n",
    "                print(f\"         non-matching data, running new fit: {r}\")\n",
    "\n",
    "\n",
    "        else: # if there isn't a fit in the dictionary, run one\n",
    "            run_fit = True\n",
    "\n",
    "        if run_fit:\n",
    "            if include_fit:\n",
    "                print(f\"{i},    running fit for: {r}, {lig}\")\n",
    "                stan_data = dict(x=x, y=y, y_err=yerr, N=len(x),\n",
    "                                 log_g_min=np.log10(gmin), log_g_max=np.log10(gmax),\n",
    "                                 log_x_min=0, log_x_max=4,\n",
    "                                 sensor_n_alpha=sensor_n_alpha,\n",
    "                                 sensor_n_beta=sensor_n_beta)\n",
    "                stan_init = init_stan_fit(x, y)\n",
    "                stan_fit = stan_model.sample(data=stan_data, \n",
    "                                             iter_warmup=int(stan_iter/2),\n",
    "                                             iter_sampling=int(stan_iter/2), \n",
    "                                             inits=stan_init, \n",
    "                                             chains=4,\n",
    "                                             adapt_delta=0.995, \n",
    "                                             max_treedepth=15)\n",
    "                cytom_Hill_fits[(r, lig)] = (x, y, yerr, stan_fit)\n",
    "                stan_utility.check_all_diagnostics(stan_fit)\n",
    "                display(stan_fit.summary())\n",
    "\n",
    "            else:\n",
    "                print(f'{i},    no data for {r}')\n",
    "                \n",
    "with open(pickle_file, 'wb') as file:\n",
    "    pickle.dump(cytom_Hill_fits, file)\n",
    "                \n",
    "with open(model_pickle_file, 'wb') as file:\n",
    "    pickle.dump(stan_model, file)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill_funct(x, low, high, mid, n):\n",
    "    return low + (high-low)*( x**n )/( mid**n + x**n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hill_header = ['G0', 'Ginf', 'EC50', 'n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hill_params = ['log_low_level', 'log_high_level', 'log_IC_50', 'sensor_n', 'log_high_low_ratio']\n",
    "param_names = ['log_g0', 'log_ginf', 'log_ec50', 'n', 'log_ginf_g0_ratio']\n",
    "popt_list = []\n",
    "perr_list = []\n",
    "var_list = []\n",
    "lig_list = []\n",
    "for k in cytom_Hill_fits.keys():\n",
    "    var = k[0]\n",
    "    lig = k[1]\n",
    "    \n",
    "    var_list.append(var)\n",
    "    lig_list.append(lig)\n",
    "    if (var, lig) in cytom_Hill_fits.keys():\n",
    "        x, y, yerr, stan_fit = cytom_Hill_fits[(var, lig)]\n",
    "        popt = [ np.mean(stan_fit.stan_variable(param)) for param in hill_params ] #low, high, mid, n\n",
    "        perr = [ np.std(stan_fit.stan_variable(param)) for param in hill_params ] #low, high, mid, n\n",
    "    else:\n",
    "        pass\n",
    "        #popt = [np.nan]*3\n",
    "        #perr = [np.nan]*3\n",
    "    popt_list.append(popt)\n",
    "    perr_list.append(perr)\n",
    "\n",
    "variant_table = pd.DataFrame({'variant':var_list, \"ligand\":lig_list})\n",
    "\n",
    "for par, x, xerr in zip(param_names, np.array(popt_list).transpose(), np.array(perr_list).transpose()):\n",
    "    variant_table[par] = x\n",
    "    variant_table[f'{par}_err'] = xerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variant_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob('*.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = 'Ligafy_cytom_Hill_fit_results_mean.pkl'\n",
    "with open(pickle_file, 'wb') as file:\n",
    "    pickle.dump(variant_table, file)\n",
    "\n",
    "variant_table.to_csv(pickle_file.replace('.pkl', '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hill_params = ['low_level', 'high_level', 'IC_50', 'sensor_n', 'log_high_low_ratio']\n",
    "param_names = ['G0', 'Ginf', 'EC50', 'n', 'Ginf_G0_ratio', 'Gmax', 'Gmax_G0_ratio']\n",
    "popt_list = []\n",
    "perr_list = []\n",
    "var_list = []\n",
    "lig_list = []\n",
    "for k in cytom_Hill_fits.keys():\n",
    "    var = k[0]\n",
    "    lig = k[1]\n",
    "    \n",
    "    var_list.append(var)\n",
    "    lig_list.append(lig)\n",
    "    if (var, lig) in cytom_Hill_fits.keys():\n",
    "        x, y, yerr, stan_fit = cytom_Hill_fits[(var, lig)]\n",
    "        popt = [ np.mean(stan_fit.stan_variable(param)) for param in hill_params[:-1] ] #low, high, mid, n\n",
    "        perr = [ np.std(stan_fit.stan_variable(param)) for param in hill_params[:-1] ] #low, high, mid, n\n",
    "        \n",
    "        param = hill_params[-1]\n",
    "        popt.append(np.mean(10**stan_fit.stan_variable(param)))\n",
    "        perr.append(np.std(10**stan_fit.stan_variable(param)))\n",
    "        \n",
    "        gmax = stan_fit.stan_variable('max_level')\n",
    "        popt.append(np.quantile(gmax, 0.05))\n",
    "        perr.append(np.nan)\n",
    "        \n",
    "        g0 = stan_fit.stan_variable('low_level')\n",
    "        popt.append(np.quantile(gmax/g0, 0.05))\n",
    "        perr.append(np.nan)\n",
    "    else:\n",
    "        pass\n",
    "        #popt = [np.nan]*3\n",
    "        #perr = [np.nan]*3\n",
    "    popt_list.append(popt)\n",
    "    perr_list.append(perr)\n",
    "\n",
    "variant_table_2 = pd.DataFrame({'variant':var_list, \"ligand\":lig_list})\n",
    "\n",
    "for par, x, xerr in zip(param_names, np.array(popt_list).transpose(), np.array(perr_list).transpose()):\n",
    "    variant_table_2[par] = x\n",
    "    variant_table_2[f'{par}_err'] = xerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variant_table_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params = ['G0', 'Ginf', 'EC50', 'Ginf_G0_ratio', 'n']\n",
    "for p in save_params:\n",
    "    print(p, max(variant_table_2[f'{p}_err']/variant_table_2[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = 'Ligafy_cytom_Hill_fit_results_mean_non_log.pkl'\n",
    "\n",
    "variant_table_2.to_csv(pickle_file.replace('.pkl', '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lower limits on EC50 (for dose-response curves with high EC50):\n",
    "for k in cytom_Hill_fits.keys():\n",
    "    var = k[0]\n",
    "    lig = k[1]\n",
    "    x, y, yerr, stan_fit = cytom_Hill_fits[(var, lig)]\n",
    "    ec50 = stan_fit.stan_variable('IC_50')\n",
    "    if np.mean(ec50) > max(x)/10:\n",
    "        lim = np.quantile(ec50, 0.05)\n",
    "        print(f'{var}, {lig}, {lim:.0f}, {max(x)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_summary\n",
    "x_max = df.inducerConcentration.max()\n",
    "df = df[df.inducerConcentration>0]\n",
    "x_min = df.inducerConcentration.min()\n",
    "x_fit = [0] + list(np.logspace(np.log10(x_min/2), np.log10(x_max*2), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot data with fits to check fit quality\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 4]\n",
    "\n",
    "\n",
    "for var in np.unique(full_summary['variant']):\n",
    "    df_0 = full_summary\n",
    "    df_0 = df_0[~df_0.is_bad_rep]\n",
    "    df_0 = df_0[df_0.variant==var]\n",
    "        \n",
    "    lig_list = np.unique(df_0.inducerId)\n",
    "    lig_list = lig_list[lig_list!='none']\n",
    "    \n",
    "    for lig in lig_list:\n",
    "        df = df_0\n",
    "        df = df[(df.inducerId==lig)|(df.inducerId=='none')]\n",
    "        df_non_zer = df[(df.inducerId==lig)]\n",
    "        \n",
    "        if ((var, lig) in cytom_Hill_fits):# and (len(df_non_zer)>1):\n",
    "            fig, axs = plt.subplots(1, 2)\n",
    "            fig.suptitle(f'{var}, {lig}', size=20)\n",
    "            \n",
    "            variant_row = variant_table[(variant_table.variant==var)&(variant_table.ligand==lig)].iloc[0]\n",
    "            hill_params = [variant_row[p] for p in ['log_g0', 'log_ginf', 'log_ec50', 'n']]\n",
    "            hill_params = [10**p for p in hill_params[:3]] + hill_params[3:]\n",
    "            y_fit = hill_funct(x_fit, *hill_params)\n",
    "            for ax in axs:\n",
    "                x = df.inducerConcentration\n",
    "                y = df['mean'] - non_fluorescent_mean\n",
    "                yerr = df['mean_err']*np.mean(cytom_Hill_fits[(var, lig)][-1].stan_variable('sigma'))\n",
    "                ax.errorbar(x, y, yerr, fmt='o', ms=10)\n",
    "                \n",
    "                ax.plot(x_fit, y_fit)\n",
    "\n",
    "            for ax in axs:\n",
    "                ax.set_xscale('symlog', linthresh=min(x[x>0])/4);\n",
    "                ax.set_xlabel(f'[{lig}] (umol/L)')\n",
    "            ax.set_yscale('log');\n",
    "        else:\n",
    "            #pass\n",
    "            print(f'No fit for {var}, {lig}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gsf_ims]",
   "language": "python",
   "name": "conda-env-gsf_ims-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
